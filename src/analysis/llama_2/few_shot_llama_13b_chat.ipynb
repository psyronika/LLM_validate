{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde61e5e-e9cc-4399-a8d5-048d00697c5a",
   "metadata": {},
   "source": [
    "# Llama 13b for validation task\n",
    "\n",
    "* input: dataframe with text pairs and additional info LLama 2 should use to make informed decision\n",
    "* output: dataframe with additional columns: topic, topic match evalutation, topic match classification, news event, news event match evaluation, news event match classification, final classification on topic-level and news event level matching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c86ae2-8644-451b-855d-9a4625b6b94d",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629e3acb-b512-4f07-bd44-ad67fa7d8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96555da-98cc-426b-b3a6-517a3e6abe8f",
   "metadata": {},
   "source": [
    "## Load model through Huggingface pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec9f8ed-1605-49f4-a9f1-0b4f356301ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/volume_2/python_3.10/lib/python3.10/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f16c3d9b79345c793bf6a7368d579ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "hf_auth_file = '../../analysis/hf_auth.txt'\n",
    "\n",
    "# Read the API token from the file\n",
    "with open(hf_auth_file, \"r\") as file:\n",
    "    hf_auth = file.read().strip()  # Remove leading/trailing whitespaces\n",
    "\n",
    "\n",
    "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")\n",
    "\n",
    "# begin initializing HF items, need auth token for these\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    config=model_config,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    use_auth_token=hf_auth\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "799a7a2f-9784-403d-9e23-71e6b2d1f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/volume_2/python_3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=hf_auth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a4c1b09-5715-46e7-88eb-822b2b2fa4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA A10\n",
      "Memory Usage: 3.559241771697998 GB\n",
      "Max Memory Usage: 3.604752540588379 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(device))\n",
    "print(\"Memory Usage:\", torch.cuda.memory_allocated(device) / 1024 ** 3, \"GB\")\n",
    "print(\"Max Memory Usage:\", torch.cuda.max_memory_allocated(device) / 1024 ** 3, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c404a3-5ee9-429e-9b64-1e1b4d60f72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.clear_autocast_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "b54484da-e4b1-457b-8c36-8d50e6ee2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max we do not want any randomness here as we want the model to stick to the prompt as closely as possible\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f474429-5dd0-4991-9975-352fa8b0d7bc",
   "metadata": {},
   "source": [
    "### Read in file\n",
    "\n",
    "This is a file for prompt engineering that we devide into smaller samples for tuning our prompts. There are 100 rows in this data and we split them up into 20, 5 row dfs for quick testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "455618db-c310-4aa4-a33c-85f30770ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to navigate up 'n' levels\n",
    "def navigate_up(current_directory, levels):\n",
    "    for _ in range(levels):\n",
    "        current_directory = os.path.dirname(current_directory)\n",
    "    return current_directory\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Specify the number of levels to navigate up (4 levels in this case)\n",
    "levels_to_navigate = 4\n",
    "\n",
    "# Navigate up 'levels_to_navigate' folders\n",
    "parent_directory = navigate_up(current_directory, levels_to_navigate)\n",
    "\n",
    "# Define the path to the data file\n",
    "file_path = os.path.join(parent_directory, 'newspaper_data', 'sample_1percent.csv')\n",
    "\n",
    "# Now you can open and read the CSV file using pandas\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "f8a58d5f-caf5-44c4-aff5-bd7fbd6e4f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similarity_Score</th>\n",
       "      <th>Text1</th>\n",
       "      <th>Text2</th>\n",
       "      <th>Group</th>\n",
       "      <th>Date1</th>\n",
       "      <th>Date2</th>\n",
       "      <th>Publisher1</th>\n",
       "      <th>Publisher2</th>\n",
       "      <th>ID1</th>\n",
       "      <th>ID2</th>\n",
       "      <th>proper_nouns1</th>\n",
       "      <th>proper_nouns2</th>\n",
       "      <th>keywords1</th>\n",
       "      <th>keywords2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.765668</td>\n",
       "      <td>Alle pijlen zijn gericht op Rutte in RTL-debat...</td>\n",
       "      <td>Helft van de Forum-stemmers ziet complot; De h...</td>\n",
       "      <td>high</td>\n",
       "      <td>2021-03-01 00:00:00</td>\n",
       "      <td>2021-03-01 00:00:00</td>\n",
       "      <td>De Volkskrant</td>\n",
       "      <td>Trouw</td>\n",
       "      <td>3285226</td>\n",
       "      <td>3285337</td>\n",
       "      <td>Radio 1, RTL, VVD, kernboodschap gelieve, Sigr...</td>\n",
       "      <td>Forum voor Democratie, Ipsos</td>\n",
       "      <td>['lijsttrekkersdebat', 'premiersdebat', 'paree...</td>\n",
       "      <td>['ipsos', 'coronavirus', 'gefabriceerd', 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.683993</td>\n",
       "      <td>Hoogste bestuursrechter liet forse steken vall...</td>\n",
       "      <td>Stelling 3: Om de klimaatdoelen te halen moet ...</td>\n",
       "      <td>medium</td>\n",
       "      <td>2021-02-28 00:00:00</td>\n",
       "      <td>2021-02-28 22:22:56</td>\n",
       "      <td>Het Financieele Dagblad</td>\n",
       "      <td>NOS liveblog</td>\n",
       "      <td>3290695</td>\n",
       "      <td>3287474</td>\n",
       "      <td>Andr Bosman, VVD, Tweede Kamer, Raad van State...</td>\n",
       "      <td>VVD, Poetin</td>\n",
       "      <td>['overheidsinstantie', 'kinderopvangtoeslagen'...</td>\n",
       "      <td>['klimaatdoelen', 'kerncentrales', 'rusland', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.848039</td>\n",
       "      <td>Planbureau: vertrek bedrijven reëel risico bij...</td>\n",
       "      <td>Baudet: corona niet bewust wereld in geslinger...</td>\n",
       "      <td>high</td>\n",
       "      <td>2021-03-01 00:00:00</td>\n",
       "      <td>2021-03-01 10:34:06</td>\n",
       "      <td>Het Financieele Dagblad</td>\n",
       "      <td>NOS nieuws</td>\n",
       "      <td>3290604</td>\n",
       "      <td>6290556</td>\n",
       "      <td>Planbureau, Planbureau voor de Leefomgeving, P...</td>\n",
       "      <td>Baudet, Forum voor Democratie-voorman Thierry ...</td>\n",
       "      <td>['broeikasgasuitstoot', 'klimaatwinst', 'leefo...</td>\n",
       "      <td>['virussen', 'chinees', 'ebolavirus', 'ingesto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.805225</td>\n",
       "      <td>Recht op reparatie van apparatuur komt steeds ...</td>\n",
       "      <td>Niet een lijsttrekker, maar een kiezer brengt ...</td>\n",
       "      <td>high</td>\n",
       "      <td>2021-03-01 00:00:00</td>\n",
       "      <td>2021-03-01 00:00:00</td>\n",
       "      <td>Het Financieele Dagblad</td>\n",
       "      <td>NRC Handelsblad</td>\n",
       "      <td>3290567</td>\n",
       "      <td>3285627</td>\n",
       "      <td>REPAIR, CAF S Jeroen Groot, Philips, Leenman, ...</td>\n",
       "      <td>Mark Rutte, RTL, Mark Rutte, Mark Rutte, Geert...</td>\n",
       "      <td>['verwarmingselement', 'reparateurs', 'koffiez...</td>\n",
       "      <td>['lijsttrekkersdebat', 'toeslagenaffaire', 'on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.631177</td>\n",
       "      <td>Dode en gewonde door zuurstofexplosie corona-a...</td>\n",
       "      <td>Wilders in de schijnwerpers; Wilders in de sch...</td>\n",
       "      <td>medium</td>\n",
       "      <td>2021-02-27 22:39:37</td>\n",
       "      <td>2021-03-01 00:00:00</td>\n",
       "      <td>NOS liveblog</td>\n",
       "      <td>De Telegraaf</td>\n",
       "      <td>3287529</td>\n",
       "      <td>3286364</td>\n",
       "      <td>Oekra, Twintig, Oekra</td>\n",
       "      <td>Wilders, Mark Rutte, Sigrid Kaag D66, Wilders,...</td>\n",
       "      <td>['tsjernivtsi', 'zaporizja', 'zuurstofexplosie...</td>\n",
       "      <td>['diversiteitsquota', 'ronald', 'rtl', 'zit', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Similarity_Score                                              Text1  \\\n",
       "5          0.765668  Alle pijlen zijn gericht op Rutte in RTL-debat...   \n",
       "6          0.683993  Hoogste bestuursrechter liet forse steken vall...   \n",
       "7          0.848039  Planbureau: vertrek bedrijven reëel risico bij...   \n",
       "8          0.805225  Recht op reparatie van apparatuur komt steeds ...   \n",
       "9          0.631177  Dode en gewonde door zuurstofexplosie corona-a...   \n",
       "\n",
       "                                               Text2   Group  \\\n",
       "5  Helft van de Forum-stemmers ziet complot; De h...    high   \n",
       "6  Stelling 3: Om de klimaatdoelen te halen moet ...  medium   \n",
       "7  Baudet: corona niet bewust wereld in geslinger...    high   \n",
       "8  Niet een lijsttrekker, maar een kiezer brengt ...    high   \n",
       "9  Wilders in de schijnwerpers; Wilders in de sch...  medium   \n",
       "\n",
       "                 Date1                Date2               Publisher1  \\\n",
       "5  2021-03-01 00:00:00  2021-03-01 00:00:00            De Volkskrant   \n",
       "6  2021-02-28 00:00:00  2021-02-28 22:22:56  Het Financieele Dagblad   \n",
       "7  2021-03-01 00:00:00  2021-03-01 10:34:06  Het Financieele Dagblad   \n",
       "8  2021-03-01 00:00:00  2021-03-01 00:00:00  Het Financieele Dagblad   \n",
       "9  2021-02-27 22:39:37  2021-03-01 00:00:00             NOS liveblog   \n",
       "\n",
       "        Publisher2      ID1      ID2  \\\n",
       "5            Trouw  3285226  3285337   \n",
       "6     NOS liveblog  3290695  3287474   \n",
       "7       NOS nieuws  3290604  6290556   \n",
       "8  NRC Handelsblad  3290567  3285627   \n",
       "9     De Telegraaf  3287529  3286364   \n",
       "\n",
       "                                       proper_nouns1  \\\n",
       "5  Radio 1, RTL, VVD, kernboodschap gelieve, Sigr...   \n",
       "6  Andr Bosman, VVD, Tweede Kamer, Raad van State...   \n",
       "7  Planbureau, Planbureau voor de Leefomgeving, P...   \n",
       "8  REPAIR, CAF S Jeroen Groot, Philips, Leenman, ...   \n",
       "9                              Oekra, Twintig, Oekra   \n",
       "\n",
       "                                       proper_nouns2  \\\n",
       "5                       Forum voor Democratie, Ipsos   \n",
       "6                                        VVD, Poetin   \n",
       "7  Baudet, Forum voor Democratie-voorman Thierry ...   \n",
       "8  Mark Rutte, RTL, Mark Rutte, Mark Rutte, Geert...   \n",
       "9  Wilders, Mark Rutte, Sigrid Kaag D66, Wilders,...   \n",
       "\n",
       "                                           keywords1  \\\n",
       "5  ['lijsttrekkersdebat', 'premiersdebat', 'paree...   \n",
       "6  ['overheidsinstantie', 'kinderopvangtoeslagen'...   \n",
       "7  ['broeikasgasuitstoot', 'klimaatwinst', 'leefo...   \n",
       "8  ['verwarmingselement', 'reparateurs', 'koffiez...   \n",
       "9  ['tsjernivtsi', 'zaporizja', 'zuurstofexplosie...   \n",
       "\n",
       "                                           keywords2  \n",
       "5  ['ipsos', 'coronavirus', 'gefabriceerd', 'comp...  \n",
       "6  ['klimaatdoelen', 'kerncentrales', 'rusland', ...  \n",
       "7  ['virussen', 'chinees', 'ebolavirus', 'ingesto...  \n",
       "8  ['lijsttrekkersdebat', 'toeslagenaffaire', 'on...  \n",
       "9  ['diversiteitsquota', 'ronald', 'rtl', 'zit', ...  "
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the DataFrame into 20 smaller DataFrames for the sake of fast tuning of prompting, each containing 5 rows\n",
    "chunk_size = 5\n",
    "chunks = [df.iloc[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Create variables for each smaller DataFrame\n",
    "for i, chunk in enumerate(chunks):\n",
    "    globals()[f'df{i + 1}'] = chunk\n",
    "\n",
    "# Now you have variables df1, df2, df3, ... containing the smaller DataFrames\n",
    "# You can access and work with them as needed\n",
    "df2\n",
    "#df2\n",
    "#....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223f53a-b58e-4f8b-8288-b7afc0989552",
   "metadata": {},
   "source": [
    "#  Prompting \n",
    "\n",
    "Prompting takes shapes in many sequetial instructions. We divide the prompts themselves into system prompt, example prompt, and main prompt to geenrate a template for each subtask. We begin with the broadest level, topic-level matching task. This task is also divided into three sepearate subtasks: (1) create topic labels for eacg text, (2) compare the topic labels and texts to decide to what extent they match, and (3) based on the explanation create a single classification topic match or no topic match. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0916b9e-02e6-4ed8-9994-f1067cf8f0b2",
   "metadata": {},
   "source": [
    "## Step 1: Extract topics. \n",
    "The prompt template is based on Grootendorst, BERTopic LLama2 implementation with example from our full dataset.\n",
    "* Important to note that for each step we pass in a system prompt, give and example, and provide a main prompt that signifies the variables and content to be considered.\n",
    "* Then we create a chain from the prompt for further sequential chaining with LangChain\n",
    "* Very important here to intially extract main topic and subtopic in order to obtain clear topics. If subtopics are not requested then the model might not understand that a topic that mentions politicians and conspiracy theories belongs to the broader topic of politics and instead it may label it as conspiracy theories alone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5a1bf4-2c8c-4052-ba85-e37802f96cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of file names\n",
    "file_names = ['system_1.txt', 'main_1.txt', 'example_1.txt']\n",
    "\n",
    "# Initialize a dictionary to store the prompts\n",
    "prompts = {}\n",
    "\n",
    "# Read and store the prompts from the files\n",
    "for file_name in file_names:\n",
    "    with open(f'prompts/{file_name}', \"r\") as file:\n",
    "        prompt = file.read().strip()  # Remove leading/trailing whitespaces\n",
    "        prompts[file_name] = prompt\n",
    "\n",
    "# Combine the prompts\n",
    "prompt_1 = prompts['system_1.txt'] + prompts['example_1.txt'] + prompts['main_1.txt']\n",
    "\n",
    "# Create a PromptTemplate instance\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text1\", \"text2\", 'proper_nouns1', 'proper_nouns2', 'keywords1', 'keywords2'],\n",
    "    template=prompt_1\n",
    ")\n",
    "\n",
    "# Create the LLMChain instance\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_template, output_key=\"topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "e22ef277-6196-42ed-9f66-562bc5451c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the system prompt. It describes information given to all conversations\n",
    "# This system prompt will be\n",
    "system_prompt_1 = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for labeling topics. A \"topic\" is a fundamental subject or theme that encompasses all aspects related to a particular area of interest or discussion. \n",
    "A topic serves as the overarching framework for exploring and discussing various facets within that subject. A topic comprises of a main topic and a subtopic. A main topic is an overarching theme, a subtopic is a more specific thematic or content-based divisions within a broader main topic. All main topics are labeled Politics if the documents' keywords and proper nouns relate to politics. For instance if the text discusses the economy but a politician, party, or government is mentioned either in the text or in the keywords then it should be categorized as Politics and not Economy. \\n\n",
    "Main topic: Politics; Subtopic: Elections and campaigns\n",
    "Main topic: Economy; Subtopic: Interest rates\n",
    "Main topic: Health; Subtopic: Mental health\n",
    "Main topic: Entertainment; Subtopic: Film and Television \\n\n",
    "\n",
    "If a text mentions politics, politicians names functions, partirs, policy, or any other politics-related term, the main topic should always be Politics.\\n\n",
    "\n",
    "\n",
    "You must always return a main topic and a subtopic and nothing else in the following format: Main topic1 : Subtopic;, Main topic2: Subtopic \\n\n",
    "Do not return any notes. Only return the label and nothing more for each text.\n",
    "\n",
    "<</SYS>>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "7dfff572-4e5d-456c-a487-f3a5709fd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt demonstrating the output we are looking for\n",
    "example_prompt_1 = \"\"\"\n",
    "I have a document pair of the following texts:\\n\n",
    "- Contact met de kiezer; Deze keer zie je geen lijsttrekkers in windjacks rondlopen op markten. Kandidaat-Kamerleden moeten noodgedwongen online contact zoeken met de kiezers. Zoals de lijsttrekker van de ChristenUnie, Gert-Jan Segers, te zien is op de bovenstaande afbeelding terwijl hij vragen beantwoordt die kiezers hem stellen op het online platform Instagram. In plaats van direct met de burgers te praten, spreken politici nu voor de camera's. Livestreams op Facebook zijn ook populair. Zo had Mark Rutte dit weekend een gesprek met horeca-ondernemers en zond de VVD dat uit op Facebook. Naast de online campagne werd er dit weekend ook op de ouderwetse manier geflyerd. Maar de meeste campagnevoerende partijleden gingen niet aanbellen uit angst voor verdere verspreiding van het coronavirus. Forum voor Democratie was de enige partij die de straat op ging om campagne te voeren. Met een vrijheidskaravaan bezocht de partij Nijmegen en Venlo voor een manifestatie. Toen er meer dan tweehonderd mensen kwamen opdagen, moest burgemeester Hubert Bruls de bijeenkomst voortijdig beëindigen, hoewel deze wel was aangekondigd en aangevraagd. Een bezoeker in Venlo twitterde dat het centrale plein voor het eerst sinds carnaval vorig jaar weer vol stond met de komst van Baudet.\n",
    "- Forum voor Democratie op zoek naar extra stemmen; Terwijl andere partijen zich nauwelijks op straat vertonen, reist Forum voor Democratie stad en land af. Deze optredens trekken niet alleen de aandacht van kiezers; het Openbaar Ministerie onderzoekt nu ook of het campagneteam van Baudet de coronaregels op grote schaal heeft overtreden. Volgens getuigen werden bij een bezoek aan Urk honderden handen geschud. En dan was er nog de volmachtrel. In een live-uitzending riep Baudet zijn kijkers op om zoveel mogelijk volmachtstemmen te regelen, aangezien kiezers dit jaar niet twee, maar drie volmachtstemmen mogen uitbrengen om de kans op besmetting te verkleinen. \"Een persoon kan eigenlijk vier keer stemmen, als je maar die volmachten kunt regelen,\" zei Baudet, en dat was een enorme kans. Maar het ministerie van Binnenlandse Zaken zei: \"Ho, dat is niet de bedoeling en is niet toegestaan.\" Het campagneteam van Forum leek daar toen al achter te komen, want de suggestie om stemmen te regelen werd snel uit de video van Baudet geknipt. Baudet houdt echter wel openlijk vast aan zijn standpunt dat er een grote kans is op verkiezingsfraude, maar dan door anderen, uiteraard.\n",
    "\n",
    "The topic of each text is described by the following keywords: 'livesessies', 'vrijheidskaravaan', 'flyerende', 'facebook', 'windjacks'; besmettingskansen', 'volmachtsstemmen', 'schielijk', 'volmachten', 'baudets'\n",
    "The following proper nouns appear in each text: Gert-Jan Segers, Mark Rutte, Forum voor Democratie, Hubert Bruls, Baudet; Forum voor Democratie, Forum voor Democratie, Ministerie kijkt, Urk, Baudet, Baudet, Baudet, Baudet\n",
    "\n",
    "Based on the information about the topic above, please create a short label of the topic for each text. Only return the label and nothing more for each text in the following format:\n",
    "\n",
    "[/INST] Main topic 1: Politics; Subtopic: Elections and campaigns; Main topic 2: Politics; Subtopic: Elections, campaigns and fraud \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "7c877972-294b-487f-9aac-ff6f219a8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt_1 = \"\"\"\n",
    "[INST]\n",
    "I have a document pair of the following texts:\n",
    "{text1} and {text2}\n",
    "\n",
    "The topic of each text is described by the following keywords: {keywords1} and {keywords2}\n",
    "The following proper nouns appear in each text: {proper_nouns1}, {proper_nouns2}\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic for each text. Only return the label and nothing more for each text in the following format: Main topic 1 : Subtopic ; Main topic 2: Subtopic\n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "fc436448-e169-4fb8-8074-d545954cda23",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = system_prompt_1 + example_prompt_1 + main_prompt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "db37e1b9-405c-4f3b-b596-f74c31742551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PromptTemplate instance\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text1\", \"text2\", 'proper_nouns1', 'proper_nouns2', 'keywords1', 'keywords2'],\n",
    "    template=prompt_1\n",
    ")\n",
    "\n",
    "# Create the LLMChain instance\n",
    "chain_1 = LLMChain(llm = llm, prompt = prompt_template, output_key=\"topics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "285bb7a1-afcc-4801-84cb-34432459df81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Main topic 1: Politics; Subtopic: Elections and campaigns; Main topic 2: Politics; Subtopic: Conspiracy theories and misinformation\n",
      "\n",
      "\n",
      "\n",
      "Main topic 1: Politics; Subtopic: Government accountability and transparency\n",
      "Main topic 2: Energy and Environment; Subtopic: Climate goals and nuclear energy\n",
      "\n",
      "Main topic 1: Politics; Subtopic: Climate change and environmental policies\n",
      "Main topic 2: Health; Subtopic: Coronavirus conspiracy theories\n",
      "\n",
      "Main topic 1: Politics; Subtopic: Elections and campaigns\n",
      "Main topic 2: Society; Subtopic: Consumer rights and repairs\n",
      "\n",
      "Main topic 1: Politics; Subtopic: International relations and conflicts; Main topic 2: Politics; Subtopic: Elections and debates\n"
     ]
    }
   ],
   "source": [
    "# Test if it works\n",
    "for index, row in df2.iterrows():\n",
    "    full_text1 = row['Text1']  \n",
    "    full_text2 = row['Text2']  \n",
    "\n",
    "    input_variables = {\n",
    "        \"text1\": full_text1,\n",
    "        \"text2\": full_text2,\n",
    "        \"proper_nouns1\": row['proper_nouns1'],\n",
    "        \"proper_nouns2\": row['proper_nouns2'],\n",
    "        \"keywords1\": row['keywords1'],\n",
    "        \"keywords2\": row['keywords2'],\n",
    "    }\n",
    "\n",
    "    # Generate text using the chain\n",
    "    generated_text = chain_1.run(input_variables)\n",
    "    \n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27fe3f-2cf4-4b8e-8045-eb12d338dfdd",
   "metadata": {},
   "source": [
    "### Step 1.1: Extract main topic from topics for matching\n",
    "\n",
    "¶This is a must otherwise the chain considers subtopics as the level of match and disregards those that match on a broad level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "8493b2e7-1417-4789-9d70-8407fe74c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the system prompt. It describes information given to all conversations\n",
    "# This system prompt will b\n",
    "system_prompt_1_1 = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for extracting the main topic from a topic. \n",
    "A topic comprises of a main topic and a subtopic. A main topic is an overarching theme, a subtopic is a more specific thematic or content-based divisions within a broader main topic.\n",
    "Main topic: Business; Subtopic: Interest rates\n",
    "Main topic: Health; Subtopic: Mental health\n",
    "Main topic: Entertainment; Subtopic: Film and Television \n",
    "Main topic: Politics; Subtopic: Elections\n",
    "\n",
    "A main topic is everything before the word 'Subtopic'\n",
    "Given a topic, you must always return the main topic nothing else in the following format: Main topic1, Main topic2: \n",
    "Only return the main topic label and nothing more for each text.\n",
    "\n",
    "<</SYS>>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "ce1fc52f-1630-44e0-9293-7710b0b4e2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt demonstrating the output we are looking for\n",
    "example_prompt_1_1 = \"\"\"\n",
    "I have a pair of topics:\n",
    "Main topic 1: Politics; Subtopic: Elections and campaigns; \\n\n",
    "Main topic 2: Politics; Subtopic: Elections, campaigns and fraud \\n\n",
    "\n",
    "Based on the information about the topic above, please extract the main topic from each topic. A main topic is everything before the word 'Subtopic'. In this case this word is Politics. Only return the label of the main topic and nothing more in the following format:\n",
    "\n",
    "[/INST] Main topic 1: Politics; Main topic 2: Politics\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "c5233e73-ba42-403b-92a2-df91a0f3b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt_1_1 = \"\"\"\n",
    "[INST]\n",
    "I have a pair of topics:\n",
    "{topics}\n",
    "\n",
    "Based on the information about the topic above, please extract the main topic from each topic. Only return the label of the main topic and nothing more in the following format:\n",
    "Main topic 1: ; Main topic 2: \n",
    "[/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "bec45d2e-f56a-400f-be92-ed91a4925e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1_1 = system_prompt_1_1 + example_prompt_1_1 + main_prompt_1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "a51d9dbb-651f-4ee3-8bc8-8910c0baec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PromptTemplate instance\n",
    "prompt_template_1 = PromptTemplate(\n",
    "    input_variables=[\"topics\"],\n",
    "    template=prompt_1_1\n",
    ")\n",
    "\n",
    "# Create the LLMChain instance\n",
    "chain_1_1 = LLMChain(llm = llm, prompt = prompt_template_1, output_key=\"main_topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1fd05-4d0c-4a6b-a5ba-7d9b640c7ff2",
   "metadata": {},
   "source": [
    "## Step 2: Evaluate topic level match\n",
    "Compare the topics of the text pairs and made eveluation about the match level  \n",
    "\n",
    "* We create a second chain for this task that uses the texts as well as the extrcated topics as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "d2e9a0db-7285-4a78-a12a-036c4a1f99a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the system prompt from the default one to a specific one in order to focus the model on a single task. \n",
    "# This system prompt will be\n",
    "system_prompt_2 = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful, and honest assistant for comparing the main topics of two texts. In this comparison, a match is solely based on the main topic and nothing else.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "711b57fe-e781-4578-8285-40a97fd56803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt demonstrating the output we are looking for\n",
    "example_prompt_2 = \"\"\"\n",
    "\n",
    "The main topic of each text is described by the following labels:\n",
    "\n",
    "Main topic 1: Politics;  \n",
    "Main topic 2: Politics; \n",
    "\n",
    "\n",
    "Based on the information about the main topics above, please write a short evaluation about whether the two texts match on a main topic level. Make sure to only return the evaluation and nothing more in the following format:\n",
    "\n",
    "[/INST] Topic Evaluation: Yes, the two texts match on a main topic level because both texts touch upon the broader context of Politics as seen by their main Topic. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "6b17947b-70af-4abd-9ba4-0084ea40bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main prompt describing the task once more and adding the input variables to be considered\n",
    "main_prompt_2 = \"\"\"\n",
    "[INST]\n",
    "\n",
    "The main topic of each text is the following: \n",
    "{main_topic}\n",
    "\n",
    "Based on the information about the topics above, please write a short evaluation about whether the two texts match on a main topic level. Make sure to only return the evaluation and nothing more in the following format:\n",
    "Topic Evaluation:\n",
    "[/INST] \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "19600ef7-e635-461f-9e2c-dba231419966",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 = system_prompt_2 + example_prompt_2 + main_prompt_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "d4dd2a0c-56c3-4d95-b348-b15a00689bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_2 = PromptTemplate(input_variables=[\"main_topic\"], template=prompt_2, batch_size=32, max_iterations = 1)\n",
    "chain_2 = LLMChain(llm = llm, prompt = prompt_template_2, output_key=\"topic_evaluation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940ddc7-73c3-41f7-a3d4-0dbdaf8dc334",
   "metadata": {},
   "source": [
    "## Step 3: Create classification label based on evaluation\n",
    "Provide single label for the match level\n",
    "\n",
    "* We create a third chain for this task that uses the texts as well as the extracted topics and evaluation as input.\n",
    "* Labels: topic match, not topic match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "fecbc38c-b157-441f-a6f6-b682ef673a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the system prompt. It describes information given to all conversations\n",
    "# This system prompt will be\n",
    "system_prompt_3 = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for classifying whether two texts match on a main topic level based on an evaluation provided. \n",
    "\n",
    "At each request excplicitly assign one of the two labels below. \n",
    "0 - no match\n",
    "1 - topic match\n",
    "\n",
    "Make sure you to only return the label and nothing else.\n",
    "<</SYS>>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "28398820-cab0-4adb-b95d-be13c983f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt demonstrating the output we are looking for\n",
    "example_prompt_3 = \"\"\"\n",
    "\n",
    "The following evaluation describes the topic match level:\n",
    "Yes, the two texts match on a main topic level. Both texts touch upon the broader context of Politics. \n",
    "Based on this information, please assign either '0 - no match' or '1 - topic match'. Make sure to only return the label and nothing more in the following format:\n",
    "\n",
    "[/INST]: 1 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "ea469ecd-17db-422d-ae07-f82c7b2cdf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags\n",
    "main_prompt_3 = \"\"\"\n",
    "[INST]\n",
    "\n",
    "The following evaluation describes the topic match level:\n",
    "{topic_evaluation}\n",
    "\n",
    "Based on this information, please assign either '0 - no match' or '1 - topic match'. Make sure to only return the label and nothing more in the following format:\n",
    "[/INST] \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "7b78d994-404d-43de-a5e2-12278ec47538",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_3 = system_prompt_3 + example_prompt_3 + main_prompt_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "83565f57-c842-4cb5-8be7-481b82c8cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_3 = PromptTemplate(input_variables=[ \"topic_evaluation\"], template=prompt_3, batch_size=32, max_iterations = 1)\n",
    "chain_3 = LLMChain(llm = llm, prompt = prompt_template_3, output_key=\"match_topic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a661c-8dac-4a55-8cea-985a6172e43f",
   "metadata": {},
   "source": [
    "## Step 4: Identify news events\n",
    "* we ask the model the identify the news event described in each text\n",
    "* input data remains the same\n",
    "* this is in preparation of assessing news event level matching similar to topic level matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "8ab4b2cc-a97a-4c4f-be0d-401b552e2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the system prompt. It describes information given to all conversations\n",
    "# This system prompt will be\n",
    "system_prompt_4 = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for idenitifying the news event described in a pair of documents. \n",
    "News events are specific events that lead to news coverage, such as a specific debate on a specific day in a specific parliament, a specific accident, or a specific football match. They can be covered by one or more articles in one or more outlets, but relate to one specific and identifiable event and are thus much more fine-grained than news topics, issues, or news categories.\n",
    "News events can span over multiple days but not more than 10 days. Therefore articles that cover the same news event are published within the same few hours and in the course of a few days. \n",
    "\n",
    "Provide your answer as an explanation in maximum 100 tokens. Make sure you to only return the news event identified and nothing else. Be very specific, name actors and places where possible. \n",
    "<</SYS>>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "fd766de4-6396-49c6-aec1-0c9695528c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt demonstrating the output we are looking for\n",
    "example_prompt_4 = \"\"\"\n",
    "I have a document pair of the following texts:\n",
    "- Contact met de kiezer; Deze keer zie je geen lijsttrekkers in windjacks rondlopen op markten. Kandidaat-Kamerleden moeten noodgedwongen online contact zoeken met de kiezers. Zoals de lijsttrekker van de ChristenUnie, Gert-Jan Segers, te zien is op de bovenstaande afbeelding terwijl hij vragen beantwoordt die kiezers hem stellen op het online platform Instagram. In plaats van direct met de burgers te praten, spreken politici nu voor de camera's. Livestreams op Facebook zijn ook populair. Zo had Mark Rutte dit weekend een gesprek met horeca-ondernemers en zond de VVD dat uit op Facebook. Naast de online campagne werd er dit weekend ook op de ouderwetse manier geflyerd. Maar de meeste campagnevoerende partijleden gingen niet aanbellen uit angst voor verdere verspreiding van het coronavirus. Forum voor Democratie was de enige partij die de straat op ging om campagne te voeren. Met een vrijheidskaravaan bezocht de partij Nijmegen en Venlo voor een manifestatie. Toen er meer dan tweehonderd mensen kwamen opdagen, moest burgemeester Hubert Bruls de bijeenkomst voortijdig beëindigen, hoewel deze wel was aangekondigd en aangevraagd. Een bezoeker in Venlo twitterde dat het centrale plein voor het eerst sinds carnaval vorig jaar weer vol stond met de komst van Baudet.\n",
    "- Forum voor Democratie op zoek naar extra stemmen; Terwijl andere partijen zich nauwelijks op straat vertonen, reist Forum voor Democratie stad en land af. Deze optredens trekken niet alleen de aandacht van kiezers; het Openbaar Ministerie onderzoekt nu ook of het campagneteam van Baudet de coronaregels op grote schaal heeft overtreden. Volgens getuigen werden bij een bezoek aan Urk honderden handen geschud. En dan was er nog de volmachtrel. In een live-uitzending riep Baudet zijn kijkers op om zoveel mogelijk volmachtstemmen te regelen, aangezien kiezers dit jaar niet twee, maar drie volmachtstemmen mogen uitbrengen om de kans op besmetting te verkleinen. \"Een persoon kan eigenlijk vier keer stemmen, als je maar die volmachten kunt regelen,\" zei Baudet, en dat was een enorme kans. Maar het ministerie van Binnenlandse Zaken zei: \"Ho, dat is niet de bedoeling en is niet toegestaan.\" Het campagneteam van Forum leek daar toen al achter te komen, want de suggestie om stemmen te regelen werd snel uit de video van Baudet geknipt. Baudet houdt echter wel openlijk vast aan zijn standpunt dat er een grote kans is op verkiezingsfraude, maar dan door anderen, uiteraard.\\n\n",
    "\n",
    "The following keywords appear in each text: 'livesessies', 'vrijheidskaravaan', 'flyerende', 'facebook', 'windjacks'; besmettingskansen', 'volmachtsstemmen', 'schielijk', 'volmachten', 'baudets'\n",
    "The following proper nouns appear in each text: Gert-Jan Segers, Mark Rutte, Forum voor Democratie, Hubert Bruls, Baudet; Forum voor Democratie, Forum voor Democratie, Ministerie kijkt, Urk, Baudet, Baudet, Baudet, Baudet\n",
    "\n",
    "The topic of each text is the following:\n",
    "Main topic 1: Politics; Subtopic: Elections and campaigns; \\n\n",
    "Main topic 2: Politics; Subtopic: Elections, campaigns and fraud \\n\n",
    "\n",
    "Based on the information above, please identify the news events that describe the texts. Be as specific as possible. Make sure to only return the events and nothing more for each text in the following format:\n",
    "\n",
    "[/INST] Event 1: Most political parties are shifting their campaign activity stategies due to the COVID-19 pandemic; Event 2: Forum voor Democratie party's campaign activities violate COVID-19 regulations while other parties have more pandemic-proof startegies. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "728dc550-3900-44bd-8060-4f2596dfb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example prompt demonstrating the output we are looking for\n",
    "main_prompt_4 = \"\"\"\n",
    "\n",
    "I have a document pair of the following texts:\n",
    "{text1} and {text2}\n",
    "\n",
    "The following keywords appear in each text: {keywords1} and {keywords2}\n",
    "The following proper nouns appear in each text: {proper_nouns1}, {proper_nouns2}\n",
    "\n",
    "The topic of each text is the following:\n",
    "{topics}\n",
    "\n",
    "Based on the information above, please identify the news events that describe the texts. Make sure to only return the news events and nothing more in the following format:\n",
    "[/INST] \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "2c2242af-37bc-4482-8748-f315ed60910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_4 = system_prompt_4 + example_prompt_4 + main_prompt_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "c376673a-c2c9-4712-96d8-88e5a1c0c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_4 = PromptTemplate(input_variables=[\"text1\", \"text2\", \"proper_nouns1\", \"proper_nouns2\", 'keywords1', 'keywords2', \"topics\" ], template=prompt_4, batch_size=32, max_iterations = 1)\n",
    "chain_4 = LLMChain(llm = llm, prompt = prompt_template_4, output_key=\"news_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ed895-7395-45da-86ee-57d2937d2a67",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate news event level match\n",
    "\n",
    "* we ask the model to compare the news events identified on whether they match \n",
    "* input data remains the same plus the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "285193cb-5e60-497a-8e52-04a74365da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the system prompt. It describes information given to all conversations\n",
    "# This system prompt will be\n",
    "system_prompt_5 = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for evaluating whether two texts pertain to the same news event.\n",
    "News events are comprised of specific events that lead to news coverage around a news story, such as a specific debate on a specific day in a specific parliament, a specific accident, or a specific football match. \\n\n",
    "They can be covered by one or more articles in one or more outlets, but relate to one specific and identifiable event and are thus much more fine-grained than news topics, issues, or news categories.\\n\n",
    "News events can span over multiple days but not more than 10 days. Therefore articles that cover the same news event are published very close in time, a matter of hours or maximum a few days. \n",
    "Different news events can also be published on the same date or on a very close date. \\n\n",
    "The most important criteria for determining whether the two texts pertain to the same news event are the events mentioned in the text. The date overlap is a secondary objective. \\n\n",
    "\n",
    "An event within a news event must refer to a specific event or related developments around the event. A news event can include various articles, reports, and updates from news outlets, all contributing to the coverage of that specific event or issue and its sorrounding aspects. \\n\n",
    "For example, the news event might revolve around the presidential election of a specific year, detailing campaign events, candidate profiles, polling data, and key issues.\n",
    "Provide your answer as an explanation in maximum 100 tokens. Make sure to only return the evaluation and nothing else.\n",
    "<</SYS>>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "2e7ade21-4985-470e-aa5d-d4e846854185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt demonstrating the output we are looking for\n",
    "example_prompt_5 = \"\"\"\n",
    "\n",
    "The news events of each text is the following:\n",
    " Event 1: Most political parties are shifting their campaign activity stategies due to the COVID-19 pandemic; \\n\n",
    " Event 2: Forum voor Democratie party's campaign activities violate COVID-19 regulations while other parties have more pandemic-proof strategies. \\n\n",
    "\n",
    "The pubishing dates of the texts is the following:\\n\n",
    "date1: 01/03/2021; date2: 01/03/2021 \\n \n",
    "\n",
    "Based on the information above, please write a short evaluation about whether the two texts match on a news event level. Make sure to only return the evaluation and nothing more in the following format:\n",
    "\n",
    "[/INST] Event Evaluation: Both texts focus on one particular news event, the election campaign and party campaign activities amid the COVID-19 pandemic which is distintive event. Both texts discuss aspects of the same election campaign, political parties and campaign strategies during the pandemic indicating that they pertain to the same news event.\n",
    "The texts were also published at a similar time and date which further indicates that they belong to the same news event. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "53c114e2-b556-4280-aecf-b755377b6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt demonstrating the output we are looking for\n",
    "main_prompt_5 = \"\"\"\n",
    "\n",
    "The news events of each text is the following:\n",
    "{news_events}\n",
    "\n",
    "The pubishing dates of the texts is the following:\n",
    "{date1} and {date2}\n",
    "\n",
    "Based on the information above, please write a short evaluation about whether the two texts match on a news event level. Make sure to only return the evaluation and nothing more in the following format:\n",
    "[/INST] \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "91199987-0633-49a5-8335-34d85c093628",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_5 = system_prompt_5 + example_prompt_5 + main_prompt_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "5e77640b-d2e2-408c-81e7-1bae8c728739",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_5 = PromptTemplate(input_variables=[\"news_events\", \"date1\", \"date2\"], template=prompt_5, batch_size=32, max_iterations = 1)\n",
    "chain_5 = LLMChain(llm = llm, prompt = prompt_template_5, output_key=\"event_evaluation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26b118-3760-4295-afb0-31f080ff548d",
   "metadata": {},
   "source": [
    "## Step 6: Provide single label for news event level match\n",
    "\n",
    "* we use the evalution and the events to make a classication whether there is a match or no match on the news event level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "b446524e-b4e0-4eea-9697-eb89c1063162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the system prompt. It describes information given to all conversations\n",
    "# This system prompt will be\n",
    "system_prompt_6 = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for classifying whether two texts match on news event level based on an evaluation provided. \n",
    "\n",
    "At each request excplicitly assign one of the two labels below. \n",
    "0 - no match\n",
    "1 - event match\n",
    "\n",
    "Make sure you to only return the label and nothing else.\n",
    "<</SYS>>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "51f0eeb0-7850-49f3-bd4b-87492b1473b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt demonstrating the output we are looking for\n",
    "example_prompt_6 = \"\"\"\n",
    "\n",
    "The following evaluation describes the news event match level:\n",
    "Both texts focus on one particular news event, the election campaign and party campaign activities amid the COVID-19 pandemic which is distintive event. Both texts discuss aspects of the same election campaign, political parties and campaign strategies during the pandemic indicating that they pertain to the same news event.\n",
    "The texts were also published at a similar time and date which further indicates that they belong to the same news event. \n",
    "Based on this information, please assign either '0 - no match' or '1 - topic match'. Make sure to only return the label and nothing more in the following format:\n",
    "\n",
    "[/INST]: 1 \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "3561f2bb-f6f5-40a4-b718-310d4b1c4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags\n",
    "main_prompt_6 = \"\"\"\n",
    "[INST]\n",
    "\n",
    "The following evaluation describes the news event match level:\n",
    "{event_evaluation}\n",
    "\n",
    "Based on this information, please assign either '0 - no match' or '1 -event match'. Make sure to only return the label and nothing more in the following format:\n",
    "[/INST] \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "ce8b7e06-7e4a-4d0e-bdcc-bce06c6a5b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_6 = system_prompt_6 + example_prompt_6 + main_prompt_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "c5139873-ecab-4728-ac18-92b46d9c005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_6 = system_prompt_6 + example_prompt_6 + main_prompt_6\n",
    "\n",
    "prompt_template_6 = PromptTemplate(input_variables=[ \"event_evaluation\"], template=prompt_6, batch_size=32, max_iterations = 1)\n",
    "chain_6 = LLMChain(llm = llm, prompt = prompt_template_6, output_key=\"match_event\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ff746-6bfc-42bc-87c5-9433a69ea2a6",
   "metadata": {},
   "source": [
    "## Create overall chain to combine previous chains into one big sequential chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d46bba-ffa5-4d1b-9365-4615cb191e19",
   "metadata": {},
   "source": [
    "### Run the overall chain and save the results into the df column\n",
    "\n",
    "this is to be modified based on all the new output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "9b07da48-2d22-47a0-9da9-0bd9d8336f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 0 in 19.11 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 1 in 19.77 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 2 in 19.74 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 3 in 17.64 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 4 in 20.69 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 5 in 20.11 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 6 in 16.86 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 7 in 20.45 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 8 in 21.77 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 9 in 17.09 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 10 in 20.22 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 11 in 20.43 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 12 in 19.71 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 13 in 20.47 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 14 in 20.59 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 15 in 19.04 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 16 in 18.26 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 17 in 18.03 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 18 in 18.83 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 19 in 18.11 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 20 in 18.11 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 21 in 17.24 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 22 in 20.28 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 23 in 21.52 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 24 in 18.70 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 25 in 17.80 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 26 in 25.23 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 27 in 22.21 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 28 in 20.62 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 29 in 19.59 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 30 in 19.56 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 31 in 16.85 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 32 in 19.76 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 33 in 20.09 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 34 in 18.06 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 35 in 18.36 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 36 in 19.96 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 37 in 17.26 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 38 in 19.64 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 39 in 19.84 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 40 in 20.40 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 41 in 19.68 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 42 in 18.95 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 43 in 20.21 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 44 in 22.51 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 45 in 23.13 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 46 in 18.93 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 47 in 19.00 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 48 in 21.28 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 49 in 20.39 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 50 in 18.14 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 51 in 19.47 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 52 in 21.88 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 53 in 19.34 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 54 in 22.06 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 55 in 17.15 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 56 in 19.77 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 57 in 21.51 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 58 in 17.76 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 59 in 17.72 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 60 in 19.02 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 61 in 19.75 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 62 in 17.67 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 63 in 18.80 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 64 in 23.07 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 65 in 28.94 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 66 in 19.03 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 67 in 18.63 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 68 in 20.68 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 69 in 19.20 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 70 in 19.80 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 71 in 17.87 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 72 in 17.62 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 73 in 20.47 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 74 in 20.35 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 75 in 22.60 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 76 in 17.87 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 77 in 18.44 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 78 in 18.70 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 79 in 19.10 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 80 in 20.12 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 81 in 19.49 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 82 in 21.04 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 83 in 19.53 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 84 in 19.95 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 85 in 19.49 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 86 in 20.78 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 87 in 19.37 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 88 in 20.10 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 89 in 18.78 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 90 in 19.82 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 91 in 18.58 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 92 in 17.10 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 93 in 18.28 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 94 in 20.45 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 95 in 18.92 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 96 in 21.38 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 97 in 17.50 seconds\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Processed row 98 in 19.54 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Create empty lists to collect the results\n",
    "pd.options.mode.chained_assignment = None  # Disable the warning (not recommended)\n",
    "\n",
    "topics = []\n",
    "topic_eval = []\n",
    "match_topic = []\n",
    "news_events = []\n",
    "event_eval = []\n",
    "match_event = []\n",
    "\n",
    "# Iterating over the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    full_text1 = str(row['Text1'])  \n",
    "    full_text2 = str(row['Text2'])  \n",
    "\n",
    "    input_variables = {\n",
    "        \"text1\": full_text1,\n",
    "        \"text2\": full_text2,\n",
    "        \"proper_nouns1\": row['proper_nouns1'],\n",
    "        \"proper_nouns2\": row['proper_nouns2'],\n",
    "        \"keywords1\": row['keywords1'],\n",
    "        \"keywords2\": row['keywords2'],\n",
    "        \"date1\": row['Date1'],\n",
    "        \"date2\": row['Date2']\n",
    "    }\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process the input_variables and get the results\n",
    "    #create overall chain to combine previous chains into one big sequential chain\n",
    "    overall_chain = SequentialChain(\n",
    "                  chains=[chain_1, chain_1_1, chain_2, chain_3, chain_4, chain_5, chain_6], input_variables = [\"text1\", \"text2\", \"proper_nouns1\", \"proper_nouns2\", 'keywords1', 'keywords2', 'date1', 'date2'],output_variables=[\"topics\", \"main_topic\", \"topic_evaluation\", \"match_topic\",\"news_events\",\"event_evaluation\", \"match_event\"],\n",
    "                  verbose=True )\n",
    "    \n",
    "    results = overall_chain(input_variables)\n",
    "    # Stop the timer\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    # Append results to respective lists\n",
    "    topics.append(results['topics'].strip())\n",
    "    match_topic.append(results['match_topic'].strip())\n",
    "    topic_eval.append(results['topic_evaluation'].strip())\n",
    "    news_events.append(results['news_events'].strip())\n",
    "    event_eval.append(results['event_evaluation'].strip())\n",
    "    match_event.append(results['match_event'].strip())\n",
    "\n",
    "    # Calculate and print the time taken for processing this row\n",
    "    row_processing_time = end_time - start_time\n",
    "    print(f\"Processed row {index} in {row_processing_time:.2f} seconds\")\n",
    "\n",
    "# Add new columns to the DataFrame\n",
    "df.loc[:,'Topic'] = topics\n",
    "df.loc[:,'Topic_eval'] = topic_eval\n",
    "df.loc[:,'Topic_match'] = match_topic\n",
    "df.loc[:,'News_events'] = news_events\n",
    "df.loc[:,'Event_eval'] = event_eval\n",
    "df.loc[:,'Event_match'] = match_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f859ba5c-e4be-4d7d-9583-83e2fc09192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "112e5e6b-d3b9-4db1-9ec0-578020970acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder 4 levels up where you want to save the DataFrame\n",
    "save_folder_path = os.path.join(parent_directory, 'initial_out')\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "os.makedirs(save_folder_path, exist_ok=True)\n",
    "\n",
    "# Define the full path to save the DataFrame\n",
    "save_file_path = os.path.join(save_folder_path, '13b_prompt_quality_00.csv')\n",
    "\n",
    "# Save the DataFrame to the specified path\n",
    "df.to_csv(save_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc466914-eb18-4e5e-bb59-ca4f9cf1a4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.10",
   "language": "python",
   "name": "python_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
